[
  {
    "objectID": "unidad5.html",
    "href": "unidad5.html",
    "title": "5A. Reporte escrito. Experimentos y análisis de algoritmos de intersección de conjuntos.",
    "section": "",
    "text": "Analisis de Algoritmos\nArif Narvaez de la O.\nMay 18, 2025"
  },
  {
    "objectID": "unidad5.html#procesamiento-del-conjunto-a",
    "href": "unidad5.html#procesamiento-del-conjunto-a",
    "title": "5A. Reporte escrito. Experimentos y análisis de algoritmos de intersección de conjuntos.",
    "section": "3.1 Procesamiento del Conjunto A",
    "text": "3.1 Procesamiento del Conjunto A\nEl Conjunto A consiste en pares de listas almacenadas en un archivo JSON, las cuales son procesadas para calcular sus intersecciones utilizando distintos algoritmos. A continuación,se detalla el procedimiento: 1. Carga del archivo JSON: Se lee el archivo ubicado en la ruta especificada, el cual contiene una lista de pares de listas. Estos datos se cargan en memoria utilizando la biblioteca json.\n\nPreparación de los algoritmos: Se definen funciones para cada algoritmo de intersección (ME, BY-BS, BY-B1, BY-B2 y BK), encapsuladas en un diccionario para su ejecución uniforme.\nEjecución y medición: Para cada par de listas:\n\n\nSe ordenan ambas listas en orden ascendente.\nSe ejecuta cada algoritmo, registrando el tiempo de procesamiento, el número de comparaciones y el tamaño de la intersección resultante.\n\n\nVisualización de resultados: Los datos obtenidos se almacenan en un DataFrame y se generan gr ́aficos de caja para comparar métricas como tiempo de ejecución, comparaciones y tama ̃no de la intersección.\n\n\npath_a = './archivos/postinglists-for-intersection-A-k=2.json'\n\nwith open(path_a) as f:\n    data = json.load(f)\n\nalgos = {\n    'ME': lambda a, b: merge_intersection(a, b),\n    'BY-BS': lambda a, b: baeza_yates(a, b, binary_search),\n    'BY-B1': lambda a, b: baeza_yates(a, b, doubling_search1),\n    'BY-B2': lambda a, b: baeza_yates(a, b, doubling_search2),\n    'BK': lambda a, b: barbay_kenyon([a, b])\n}\n\nresults = []\nfor i, (a, b) in enumerate(data):\n    a, b = sorted(a), sorted(b)\n    for name, func in algos.items():\n        reset_comparisons()\n        t0 = time.time()\n        res = func(a, b)\n        t1 = time.time()\n        results.append({\n            'Group': i, 'Algorithm': name,\n            'Time': t1 - t0,\n            'Comparisons': comparisons,\n            'Length': len(res)\n        })\n\ndf_A = pd.DataFrame(results)\nfor metric in ['Time', 'Comparisons', 'Length']:\n    sns.boxplot(x='Algorithm', y=metric, data=df_A)\n    plt.title(f'{metric} - Conjunto A')\n    plt.show()"
  },
  {
    "objectID": "unidad5.html#procesamiento-del-conjunto-c",
    "href": "unidad5.html#procesamiento-del-conjunto-c",
    "title": "5A. Reporte escrito. Experimentos y análisis de algoritmos de intersección de conjuntos.",
    "section": "3.3 Procesamiento del Conjunto C",
    "text": "3.3 Procesamiento del Conjunto C\nEl Conjunto C extiende el análisis a tétradas de listas (k=4), lo que permite evaluar elcomportamiento de los algoritmos en un contexto de mayor dimensionalidad. Este enfoque introduce una complejidad adicional al aumentar el número de posibles combinaciones por pares.\n\nCarga del archivo JSON: Se lee el archivo ubicado en postinglists-for-intersection-C-k=4.json, cargando t ́etradas de listas en memoria.\nEstrategias de intersección:\n\n\nBarbay-Kenyon (BK): Opera directamente sobre las cuatro listas simultáneamente, aprovechando su dise ̃no para m ́ultiples listas ordenadas.\nAlgoritmos por pares: Se aplican las mismas t ́ecnicas que en los conjuntos anteriores (ME, BY-BS, BY-B1, BY-B2), pero ahora sobre las 42 = 6 combinaciones posibles de pares (0-1, 0-2, 0-3, 1-2, 1-3, 2-3).\n\n\nMétricas y etiquetado: Cada ejecución registra:\n\n\nAlgorithm: Identificador del algoritmo y par evaluado (ej: BY-BS (0-3)).\nTime, Comparisons, Length: Tiempo, comparaciones y tama ̃no de la intersección.\n\n\nVisualización: Los gráficos de caja permiten comparar el rendimiento de BK frente a las intersecciones por pares, destacando el impacto del aumento de dimensionalidad.\n\n\npath_c = './archivos/postinglists-for-intersection-C-k=4.json'\n\nwith open(path_c) as f:\n    data = json.load(f)\n\nresults = []\nfor i, group in enumerate(data):\n    lists = [sorted(l) for l in group]\n    # BK sobre las 4 listas\n    reset_comparisons()\n    t0 = time.time()\n    res = barbay_kenyon(lists, binary_search)\n    t1 = time.time()\n    results.append({\n        'Group': i, 'Algorithm': 'BK',\n        'Time': t1 - t0,\n        'Comparisons': comparisons,\n        'Length': len(res)\n    })\n    # ME y BY sobre todos los pares\n    for idx1, idx2 in combinations(range(4), 2):\n        a, b = lists[idx1], lists[idx2]\n        for name, func in algos_pair.items():\n            reset_comparisons()\n            t0 = time.time()\n            res = func(a, b)\n            t1 = time.time()\n            results.append({\n                'Group': i, 'Algorithm': name + f' ({idx1}-{idx2})',\n                'Time': t1 - t0,\n                'Comparisons': comparisons,\n                'Length': len(res)\n            })\n\n# GRAFICAR\ndf_C = pd.DataFrame(results)\nfor metric in ['Time', 'Comparisons', 'Length']:\n    sns.boxplot(x='Algorithm', y=metric, data=df_C)\n    plt.title(f'{metric} - Conjunto C')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "unidad3.html",
    "href": "unidad3.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "#3A. Reporte escrito. Experimentos y análisis de algoritmos de ordenamiento. ####MATERIA: Análisis de algoritmos 2025-1 ####ALUMNO: ARIF NARVAEZ DE LA O"
  },
  {
    "objectID": "unidad3.html#introducción",
    "href": "unidad3.html#introducción",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "Introducción",
    "text": "Introducción\nEl análisis de algoritmos de ordenamiento es fundamental para entender cómo diferentes métodos pueden resolver un mismo problema de manera más o menos eficiente. En este trabajo, se implementarán y compararán cinco algoritmos de ordenamiento: Bubble Sort, Heapsort, Mergesort, Quicksort y la estructura de datos SkipList. El objetivo es evaluar su rendimiento en términos de tiempo de ejecución y número de operaciones realizadas, utilizando archivos JSON con diferentes niveles de perturbación.\nLos archivos JSON contienen listas de números que representan conjuntos de datos con distintos grados de desorden. A través de este análisis, se busca determinar cuál de estos algoritmos es más eficiente para ordenar grandes volúmenes de datos y cómo su rendimiento varía según el nivel de desorden inicial.\n#1- IMPORTACION DE LAS LIBRERIAS\n\nimport time\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport io\n\n#2-RUTA DE ACCESO PARA LOS ARCHIVOS\nLa variable ruta almacena la generalidad del path donde se encuentran alojados los archivos y en conjunto de un diccionario se concatenan los nombres de los archivos. La finalidad de crear un ruta por medio de un diccionario es que se presentaron problemas de dimensionalidad al querer guardar os datos cargados en una lista, por ello se opto por mejor crear una funcion que cargue y lea los archivos iterando cada una de las rutas de los archivos para despues aplicar los algoritmos que señala el ejercicio.\n\narchivos_json = {\n    \"lista_post_p016\": \"./archivos/listas-posteo-con-perturbaciones-p=016.json\",\n    \"lista_post_p032\": \"./archivos/listas-posteo-con-perturbaciones-p=032.json\",\n    \"lista_post_p064\": \"./archivos/listas-posteo-con-perturbaciones-p=064.json\",\n    \"lista_post_p128\": \"./archivos/listas-posteo-con-perturbaciones-p=128.json\",\n    \"lista_post_p256\": \"./archivos/listas-posteo-con-perturbaciones-p=256.json\",\n    \"lista_post_p512\": \"./archivos/listas-posteo-con-perturbaciones-p=512.json\"\n}\n\n#3-CARGA Y LECTURA DE LOS DATOS La función\ncargar_json_como_lista\nse encarga de leer un archivo JSON y convertirlo en una lista de listas, lo cual es útil para procesar datos estructurados. Primero, abre el archivo JSON en modo lectura y carga su contenido utilizando la función json.load. Luego se verifica la estructura del JSON cargado: si es un diccionario, extrae los valores (que se espera que sean listas) y los convierte en una lista de listas esta es una medida preventiva para evitar problemas con los tipos de datos a la hora de aplicar los algoritmos; posteriormente si es una lista de listas, la utiliza directamente. Si el archivo JSON no tiene una estructura válida (es decir, no es un diccionario ni una lista de listas), la función lanza una excepción ValueError indicando que la estructura no es compatible.\n\n# Función para cargar un archivo JSON como lista de listas\ndef cargar_json_como_lista(ruta_archivo):\n    with open(ruta_archivo, 'r') as archivo:\n        data = json.load(archivo)\n\n    # Verificar si el JSON es un diccionario o una lista\n    if isinstance(data, dict):\n        # Si es un diccionario, extraer las listas de los valores\n        return list(data.values())\n    elif isinstance(data, list):\n        # Si es una lista de listas, usarla directamente\n        return data\n    else:\n        raise ValueError(\"El archivo JSON no tiene una estructura válida.\")\n\n  # Funciones auxiliares para manejo de None\ndef move_nones_to_end(lst):\n    non_nones = [x for x in lst if x is not None]\n    nones = [None] * (len(lst) - len(non_nones))\n    return non_nones + nones\n\ndef move_nones_to_beginning(lst):\n    non_nones = [x for x in lst if x is not None]\n    nones = [None] * (len(lst) - len(non_nones))\n    return nones + non_nones\n\n#4-ALGORITMOS A UTILIZAR Se procede a crear un algoritmo con las operaciones necesarias para llevar a cabo los calculos solicitados, estos se declaran en una funcion para posteriormente ser llamada en otra seccion del notebook\n\ndef adaptive_bubble_sort(lst):\n    lst = move_nones_to_end(lst.copy())\n    n = len(lst)\n    operaciones = 0\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            operaciones += 1\n            if lst[j] &gt; lst[j+1]:\n                lst[j], lst[j+1] = lst[j+1], lst[j]\n                operaciones += 1\n                swapped = True\n        if not swapped:\n            break\n    return lst, operaciones\n\ndef heapsort(lst):\n    lst = move_nones_to_end(lst.copy())\n\n    def heapify(arr, n, i, operaciones):\n        largest = i\n        left = 2 * i + 1\n        right = 2 * i + 2\n\n        if left &lt; n and arr[left] &gt; arr[largest]:\n            largest = left\n        if right &lt; n and arr[right] &gt; arr[largest]:\n            largest = right\n        operaciones += 2\n\n        if largest != i:\n            arr[i], arr[largest] = arr[largest], arr[i]\n            operaciones += 1\n            operaciones = heapify(arr, n, largest, operaciones)\n        return operaciones\n\n    n = len(lst)\n    operaciones = 0\n    for i in range(n // 2 - 1, -1, -1):\n        operaciones = heapify(lst, n, i, operaciones)\n    for i in range(n - 1, 0, -1):\n        lst[i], lst[0] = lst[0], lst[i]\n        operaciones += 1\n        operaciones = heapify(lst, i, 0, operaciones)\n    return lst, operaciones\n\ndef optimized_mergesort(lst):\n    lst = move_nones_to_end(lst.copy())\n\n    def merge(arr, l, m, r, operaciones):\n        n1 = m - l + 1\n        n2 = r - m\n\n        # Usar slicing en lugar de copias completas\n        L = arr[l:m+1]\n        R = arr[m+1:r+1]\n\n        i = j = 0\n        k = l\n\n        while i &lt; n1 and j &lt; n2:\n            operaciones += 1\n            if L[i] &lt;= R[j]:\n                arr[k] = L[i]\n                i += 1\n            else:\n                arr[k] = R[j]\n                j += 1\n            k += 1\n\n        while i &lt; n1:\n            arr[k] = L[i]\n            i += 1\n            k += 1\n\n        while j &lt; n2:\n            arr[k] = R[j]\n            j += 1\n            k += 1\n\n        return operaciones\n\n    def sort(arr, l, r, operaciones):\n        if l &lt; r:\n            m = l + (r - l) // 2\n            operaciones = sort(arr, l, m, operaciones)\n            operaciones = sort(arr, m + 1, r, operaciones)\n            operaciones = merge(arr, l, m, r, operaciones)\n        return operaciones\n\n    operaciones = 0\n    operaciones = sort(lst, 0, len(lst) - 1, operaciones)\n    return lst, operaciones\n\ndef improved_quicksort(lst):\n    lst = move_nones_to_end(lst.copy())\n\n    def partition(arr, low, high, operaciones):\n        # Selección aleatoria del pivote\n        pivot_idx = random.randint(low, high)\n        arr[pivot_idx], arr[high] = arr[high], arr[pivot_idx]\n        pivot = arr[high]\n\n        i = low - 1\n        for j in range(low, high):\n            operaciones += 1\n            if arr[j] &lt; pivot:\n                i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n                operaciones += 1\n        arr[i + 1], arr[high] = arr[high], arr[i + 1]\n        operaciones += 1\n        return i + 1, operaciones\n\n    def sort(arr, low, high, operaciones):\n        if low &lt; high:\n            pi, operaciones = partition(arr, low, high, operaciones)\n            operaciones = sort(arr, low, pi - 1, operaciones)\n            operaciones = sort(arr, pi + 1, high, operaciones)\n        return operaciones\n\n    operaciones = 0\n    operaciones = sort(lst, 0, len(lst) - 1, operaciones)\n    return lst, operaciones\n\n# Implementación mejorada de Skip List\nclass SkipNode:\n    def __init__(self, value=None, level=0):\n        self.value = value\n        self.forward = [None] * level\n\nclass ImprovedSkipList:\n    def __init__(self, p=0.5):\n        self.p = p\n        self.max_level = 16  # Nivel máximo razonable\n        self.head = SkipNode(level=self.max_level)\n        self.level = 1\n        self.operaciones = 0\n\n    def random_level(self):\n        lvl = 1\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n            self.operaciones += 1\n        return lvl\n\n    def insert(self, value):\n        update = [None] * self.max_level\n        current = self.head\n\n        for i in range(self.level - 1, -1, -1):\n            while current.forward[i] and current.forward[i].value &lt; value:\n                current = current.forward[i]\n                self.operaciones += 1\n            update[i] = current\n            self.operaciones += 1\n\n        current = current.forward[0]\n\n        if current is None or current.value != value:\n            new_level = self.random_level()\n\n            if new_level &gt; self.level:\n                for i in range(self.level, new_level):\n                    update[i] = self.head\n                self.level = new_level\n\n            new_node = SkipNode(value, new_level)\n\n            for i in range(new_level):\n                new_node.forward[i] = update[i].forward[i]\n                update[i].forward[i] = new_node\n                self.operaciones += 1\n\n    def to_list(self):\n        result = []\n        current = self.head.forward[0]\n        while current:\n            result.append(current.value)\n            current = current.forward[0]\n        return result\n\ndef skip_list_sort(lst):\n    lst = move_nones_to_end(lst.copy())\n    skip_list = ImprovedSkipList()\n    for item in lst:\n        if item is not None:\n            skip_list.insert(item)\n    return skip_list.to_list() + [None] * (len(lst) - len(skip_list.to_list())), skip_list.operaciones\n\n#5-APLICACION DE LOS ALGORITMOS A LOS DATOS\nLa función\naplicar_algoritmo_lista\nse encarga de aplicar un algoritmo de ordenamiento específico a una lista de datos y medir tanto el tiempo de ejecución como el número de operaciones (comparaciones) realizadas. Primero registra el tiempo inicial antes de ejecutar el algoritmo. Posteriormente llama al algoritmo pasando una copia de la lista original para evitar modificaciones no deseadas. El algoritmo devuelve la lista ordenada y el número de operaciones realizadas. Finalmente, se calcula el tiempo de ejecución restando el tiempo inicial del tiempo actual.\nPor alguna razon que no se logro identificar un error relacionado con el tipo de datos de los archivos, lo que no permitia la ejecucion del codigo en algunas listas, por lo que se empleó un “Try” por si ocurre algún error durante la ejecución, se captura la excepción y se imprime un mensaje de error, devolviendo “None” para el tiempo y las operaciones para poder continuar midiendo aquellos datos a los que si se le puedan aplicar los algoritmos definidos.\n\n\"\"\" --- Funcion que aplica los algoritmos a las listas de datos ---\"\"\"\n\ndef aplicar_algoritmo_lista(lista, algoritmo, nombre_algoritmo):\n    try:\n        inicio_tiempo = time.time()\n        lista_ordenada, operaciones = algoritmo(lista.copy())\n        tiempo_ejecucion = time.time() - inicio_tiempo\n        return tiempo_ejecucion, operaciones\n    except Exception as e:\n        print(f\"Error con {nombre_algoritmo}: {e}\")\n        return None, None\n\n#6-PROCESAMIENTO DE LOS DATOS Se procede a aplicar los diferentes algoritmos de ordenamiento a los datos contenidos en los archivos previamente cargados, y almacenar los resultados en un DataFrame para su posterior análisis. Primero, se define un diccionario resultados_totales que servirá para almacenar los resultados de cada algoritmo aplicado, incluyendo el nombre del archivo, el algoritmo utilizado, el tiempo total de ejecución y el número total de operaciones realizadas. Luego, se itera sobre cada archivo JSON, cargando los datos como una lista. Para cada archivo, se aplican los algoritmos de ordenamiento (Bubble Sort, Heapsort, Mergesort, Quicksort y SkipList) a cada columna (lista) de datos, sumando el tiempo de ejecución y las operaciones realizadas. Estos resultados se almacenan en el diccionario resultados_totales. Finalmente, se crea un DataFrame a partir del diccionario y se guarda en un archivo CSV (resultados_totales.csv) para facilitar su visualización y análisis. Este proceso permite comparar el rendimiento de los algoritmos en términos de tiempo y operaciones para diferentes conjuntos de datos.\n\n# Procesamiento de datos y generación de resultados\nresultados_totales = {\n    'Archivo': [],\n    'Algoritmo': [],\n    'Tiempo Total (segundos)': [],\n    'Operaciones Totales': []\n}\n\nalgoritmos = [\n    (adaptive_bubble_sort, \"Bubble Sort Adaptativo\"),\n    (heapsort, \"Heapsort\"),\n    (optimized_mergesort, \"Mergesort Optimizado\"),\n    (improved_quicksort, \"Quicksort Mejorado\"),\n    (skip_list_sort, \"SkipList Mejorada\")\n]\n\nfor nombre_archivo, ruta_archivo in archivos_json.items():\n    print(f\"\\nProcesando archivo: {nombre_archivo}...\")\n    datos = cargar_json_como_lista(ruta_archivo)\n\n    for algoritmo, nombre_algoritmo in algoritmos:\n        tiempo_total = 0\n        operaciones_totales = 0\n\n        for lista in datos:\n            tiempo, ops = aplicar_algoritmo_lista(lista, algoritmo, nombre_algoritmo)\n            if tiempo is not None:\n                tiempo_total += tiempo\n                operaciones_totales += ops\n\n        resultados_totales['Archivo'].append(nombre_archivo)\n        resultados_totales['Algoritmo'].append(nombre_algoritmo)\n        resultados_totales['Tiempo Total (segundos)'].append(tiempo_total)\n        resultados_totales['Operaciones Totales'].append(operaciones_totales)\n\n# Crear DataFrame con los resultados\ndf_resultados_totales = pd.DataFrame(resultados_totales)\ndf_resultados_totales.to_csv('resultados_mejorados.csv', index=False)\n\n\nProcesando archivo: lista_post_p016...\n\nProcesando archivo: lista_post_p032...\n\nProcesando archivo: lista_post_p064...\n\nProcesando archivo: lista_post_p128...\n\nProcesando archivo: lista_post_p256...\n\nProcesando archivo: lista_post_p512...\n\n\n#7-ALMACENAMIENTO DE LOS DATOS Se guardan los datos obtenidos de cada uno de los archivos con los tiempos de ejecucion y comparacion de cada algoritmo para posteriormete realizar un analisis y comparacion de cada uno de ellos\n\n# Crear DataFrame con los resultados totales\ndf_resultados_totales = pd.DataFrame(resultados_totales)\n\n\ndf_resultados_totales\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n0\nlista_post_p016\nBubble Sort Adaptativo\n545.405568\n1241375084\n\n\n1\nlista_post_p016\nHeapsort\n4.832499\n7149846\n\n\n2\nlista_post_p016\nMergesort Optimizado\n2.164351\n1572444\n\n\n3\nlista_post_p016\nQuicksort Mejorado\n2.958241\n4156737\n\n\n4\nlista_post_p016\nSkipList Mejorada\n4.863472\n5007548\n\n\n5\nlista_post_p032\nBubble Sort Adaptativo\n536.773958\n1234395441\n\n\n6\nlista_post_p032\nHeapsort\n3.971302\n7143747\n\n\n7\nlista_post_p032\nMergesort Optimizado\n2.267887\n1669236\n\n\n8\nlista_post_p032\nQuicksort Mejorado\n2.167979\n4142695\n\n\n9\nlista_post_p032\nSkipList Mejorada\n6.092327\n5045098\n\n\n10\nlista_post_p064\nBubble Sort Adaptativo\n578.639286\n1298424914\n\n\n11\nlista_post_p064\nHeapsort\n3.538256\n7134135\n\n\n12\nlista_post_p064\nMergesort Optimizado\n2.021112\n1755354\n\n\n13\nlista_post_p064\nQuicksort Mejorado\n2.010129\n4238745\n\n\n14\nlista_post_p064\nSkipList Mejorada\n7.373131\n5059328\n\n\n15\nlista_post_p128\nBubble Sort Adaptativo\n570.072652\n1316731629\n\n\n16\nlista_post_p128\nHeapsort\n4.887449\n7112061\n\n\n17\nlista_post_p128\nMergesort Optimizado\n3.142073\n1844035\n\n\n18\nlista_post_p128\nQuicksort Mejorado\n3.326234\n4191140\n\n\n19\nlista_post_p128\nSkipList Mejorada\n7.178315\n5083071\n\n\n20\nlista_post_p256\nBubble Sort Adaptativo\n581.760185\n1333628216\n\n\n21\nlista_post_p256\nHeapsort\n4.757568\n7079295\n\n\n22\nlista_post_p256\nMergesort Optimizado\n1.649465\n1915028\n\n\n23\nlista_post_p256\nQuicksort Mejorado\n1.887244\n4117944\n\n\n24\nlista_post_p256\nSkipList Mejorada\n5.778150\n4975169\n\n\n25\nlista_post_p512\nBubble Sort Adaptativo\n441.719896\n1357473311\n\n\n26\nlista_post_p512\nHeapsort\n2.617562\n7032459\n\n\n27\nlista_post_p512\nMergesort Optimizado\n1.886221\n1975697\n\n\n28\nlista_post_p512\nQuicksort Mejorado\n1.953944\n4209766\n\n\n29\nlista_post_p512\nSkipList Mejorada\n6.857789\n4903654\n\n\n\n\n\n\n\n#8-VISUALIZACION DE LOS RESULTADOS Se procede a agrupar por tipo de algoritmo utilizado los tiempos de ejecucion de cada archivo para observar como interactuan los resultados conforme se incrementa el nivel de perturbacion\n\ndf_bubble_sort = df_resultados_totales[df_resultados_totales['Algoritmo'] == 'Bubble Sort']\ndf_bubble_sort\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n\n\n\n\n\n\ndf_heapsort = df_resultados_totales[df_resultados_totales['Algoritmo'] == 'Heapsort']\ndf_heapsort\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n1\nlista_post_p016\nHeapsort\n4.832499\n7149846\n\n\n6\nlista_post_p032\nHeapsort\n3.971302\n7143747\n\n\n11\nlista_post_p064\nHeapsort\n3.538256\n7134135\n\n\n16\nlista_post_p128\nHeapsort\n4.887449\n7112061\n\n\n21\nlista_post_p256\nHeapsort\n4.757568\n7079295\n\n\n26\nlista_post_p512\nHeapsort\n2.617562\n7032459\n\n\n\n\n\n\n\n\ndf_mergesort = df_resultados_totales[df_resultados_totales['Algoritmo'] == 'Mergesort']\ndf_mergesort\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n\n\n\n\n\n\ndf_quicksort = df_resultados_totales[df_resultados_totales['Algoritmo'] == 'Quicksort']\ndf_quicksort\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n\n\n\n\n\n\ndf_skip_list = df_resultados_totales[df_resultados_totales['Algoritmo'] == 'SkipList']\ndf_skip_list\n\n\n\n\n\n\n\n\nArchivo\nAlgoritmo\nTiempo Total (segundos)\nOperaciones Totales\n\n\n\n\n\n\n\n\n\n#9-CONCLUSIONES A partir de los resultados mostrados en los dataframes, se observa que:\n\nBubble Sort fue el algoritmo más lento en todos los casos, con un tiempo de ejecución significativamente mayor y un número de operaciones más elevado en comparación con los demás algoritmos. Esto era esperado debido a su complejidad algorítmica de O(n^2), lo que lo hace ineficiente para conjuntos de datos grandes.\nHeapsort y Mergesort mostraron un rendimiento similar, con tiempos de ejecución y operaciones totales considerablemente menores que Bubble Sort. Ambos algoritmos tienen una complejidad de O(nlogn), lo que los hace más eficientes para grandes volúmenes de datos.\nQuicksort fue el algoritmo más rápido en la mayoría de los casos, con un tiempo de ejecución y número de operaciones ligeramente menor que Heapsort y Mergesort. Sin embargo, su rendimiento puede variar dependiendo de la elección del pivote, aunque en este caso no se observaron problemas significativos.\nSkipList, aunque es una estructura de datos interesante y eficiente para búsquedas e inserciones, no fue tan rápida como los algoritmos de ordenamiento tradicionales en este contexto. Esto se debe a que su implementación requiere más operaciones para mantener la estructura de niveles, lo que aumenta el tiempo de ejecución y el número de operaciones.\n\n##9.1-Conclusión general Podemos concluir que, para conjuntos de datos grandes con diferentes niveles de perturbación, algoritmos como Quicksort, Heapsort y Mergesort son las mejores opciones debido a su eficiencia y estabilidad. Por otro lado, Bubble Sort y SkipList no son recomendables para este tipo de tareas, ya que su rendimiento no escala adecuadamente con el tamaño de los datos. La elección del algoritmo debe basarse en los requisitos específicos de eficiencia y estabilidad, considerando que Quicksort es generalmente la opción más rápida y eficiente.\n#REFERENCIAS 1- H. CORMEN, T., E. LEISERSON, C., L. RIVEST, R., & STEIN, C. (2022). Introduction to algorithms. MIT Press.\n2-Sedgewick, R., & Wayne, K. (2011). Algorithms Fourth Edition. Boston: Pearson Education, Inc. .\n3-GeeksforGeeks. (n.d.). Sorting algorithms. GeeksforGeeks. Recuperado el 05 de marzo de 2025, de https://www.geeksforgeeks.org/sorting-algorithms/?ref=lbp"
  },
  {
    "objectID": "unidad1.html",
    "href": "unidad1.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "#1A. Reporte escrito. Experimentos y análisis\nARIF NARVAEZ DE LA O\n#INTRODUCCION El análisis de la complejidad algorítmica es una de las áreas fundamentales en la teoría de la computación, ya que permite estimar el rendimiento de un algoritmo en función del tamaño de su entrada. Este análisis se realiza mediante las notaciones de orden de crecimiento, que nos permiten clasificar los algoritmos según su eficiencia a medida que aumenta el tamaño de la entrada (Cormen et al., 2009). Estas notaciones, como O(1), O(log n), O(n), O(n log n), O(n²), O(n³), O(a^n), O(n!) y O(n^n), representan diferentes tasas de crecimiento y ayudan a comparar la eficiencia de distintos algoritmos bajo condiciones similares (Sedgewick & Wayne, 2011).\nEn este reporte, realizamos una simulación para comparar estos órdenes de crecimiento y analizar sus tiempos de ejecución en función de diferentes tamaños de entrada. El propósito es entender cómo se comportan los algoritmos con distintos órdenes de crecimiento y cuáles son más adecuados para manejar grandes volúmenes de datos.\n\n# se importan las bibliotecas necesarias\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport math\n\nDefinimos las funciones de complejidad para cada orden de crecimiento Vamos a definir las funciones de cada orden de crecimiento que vamos a comparar.\n\ndef O1(n):\n    return np.ones_like(n)\n\ndef Ologn(n):\n    return np.log2(n)\n\ndef On(n):\n    return n\n\ndef Onlogn(n):\n    return n * np.log2(n)\n\ndef On2(n):\n    return n**2\n\ndef On3(n):\n    return n**3\n\ndef Oan(n):\n    return np.exp(n)\n\ndef Onfact(n):\n    # n! crece muy rápido, así que con valores grandes de n podría ser inabordable.\n    return np.array([math.factorial(x) for x in n])\n\ndef Onn(n):\n    return n**n\n\nGeneramos las gráficas de comparación entre los órdenes de crecimiento. Cada una de estas comparaciones será una figura.\n\nn_values_small = np.arange(1, 21)  # se limita  n entre 1 y 20 para evitar numeros muy grandes\nn_values = np.arange(1, 10000)  # para O(1), O(log n), O(n), O(n log n), O(n²), O(n³)\ny_O1 = O1(n_values)\ny_Ologn = Ologn(n_values)\ny_On = On(n_values)\ny_Onlogn = Onlogn(n_values)\ny_On2 = On2(n_values)\ny_On3 = On3(n_values)\n\n##1. O(1) vs O(log n)\nO(1) es el más eficiente, ya que su tiempo de ejecución es constante, independientemente del tamaño de la entrada. En comparación, O(log n) el tiempo crece lentamente conforme n aumenta. En los resultados de la simulación, O(1) se mantuvo constante, mientras que O(log n) creció poco a poco, aunque de forma manejable.\n\nplt.figure(figsize=(10,6))\nplt.plot(n_values, y_O1, label='O(1)', color='r')\nplt.plot(n_values, y_Ologn, label='O(log n)', color='b')\nplt.title('Comparación de O(1) vs O(log n)')\nplt.xlabel('Tamaño de entrada (n)')\nplt.ylabel('Tiempo estimado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n##2. O(n) vs O(n log n) Los algoritmos con complejidad O(n) crecen linealmente como se puede observar en la grafica. Por otro lado, O(n log n) tiene un pequeño crecimiento adicional por el término logarítmico, que se vuelve significativo a medida que n aumenta. En las simulaciones, O(n log n) empezó a mostrar tiempos mayores conforme aumentaba n, aunque no tanto como O(n²).\n\ny_On = On(n_values)\ny_Onlogn = Onlogn(n_values)\n\nplt.figure(figsize=(10,6))\nplt.plot(n_values, y_On, label='O(n)', color='r')\nplt.plot(n_values, y_Onlogn, label='O(n log n)', color='b')\nplt.title('Comparación de O(n) vs O(n log n)')\nplt.xlabel('Tamaño de entrada (n)')\nplt.ylabel('Tiempo estimado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n##3. O(n²) vs O(n³) Ambos son órdenes polinómicos, pero O(n³) crece mucho más rápido que O(n²). Aunque ambos parecen razonables para n pequeños, el crecimiento de O(n³) es más pronunciado, volviéndose mucho más lento cuando n es grande. O(n³) mostró tiempos de ejecución significativamente mayores que O(n²) en la simulación.\n\ny_On2 = On2(n_values)\ny_On3 = On3(n_values)\n\nplt.figure(figsize=(10,6))\nplt.plot(n_values, y_On2, label='O(n²)', color='r')\nplt.plot(n_values, y_On3, label='O(n³)', color='b')\nplt.title('Comparación de O(n²) vs O(n³)')\nplt.xlabel('Tamaño de entrada (n)')\nplt.ylabel('Tiempo estimado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n##4. O(a^n) vs O(n!) Tanto O(a^n) como O(n!) son de crecimiento exponencial y factorial, respectivamente, lo que significa que ambos se vuelven imprácticos rápidamente para cualquier tamaño de n relativamente grandr. En la simulación, ambos órdenes alcanzaron tiempos de ejecución tan altos que no fueron viables ni siquiera con entradas de tamaño moderado.\n\n# limitamos n porque los valores crecen muy rápido\nn_values_small = np.arange(1, 7)  # se probaron distintos valores y se considera que en el 7 se aprecia mejor el cambio del incremento entre las funciones\ny_Oan = Oan(n_values_small)\ny_Onfact = Onfact(n_values_small)\nplt.figure(figsize=(10,6))\nplt.plot(n_values_small, y_Oan, label='O(a^n)', color='r')\nplt.plot(n_values_small, y_Onfact, label='O(n!)', color='b')\nplt.title('Comparación de O(a^n) vs O(n!)')\nplt.xlabel('Tamaño de entrada (n)')\nplt.ylabel('Tiempo estimado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n##5. O(n!) vs O(n^n) Finalmente, O(n^n) crece mucho más rápido que O(n!). Ambos son tan ineficientes para tamaños grandes de entrada que en la simulación no pudieron manejarse más allá de valores pequeños de n. Esto resalta lo poco prácticos que son estos algoritmos, incluso para datos de tamaño mediano.\n\ny_Onfact = Onfact(n_values_small)\ny_Onn = Onn(n_values_small)\n\nplt.figure(figsize=(10,6))\nplt.plot(n_values_small, y_Onfact, label='O(n!)', color='r')\nplt.plot(n_values_small, y_Onn, label='O(n^n)', color='b')\nplt.title('Comparación de O(n!) vs O(n^n)')\nplt.xlabel('Tamaño de entrada (n)')\nplt.ylabel('Tiempo estimado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n##RESUMEN DE LO OBSERVADO Los resultados obtenidos en la simulación muestran que, a medida que el tamaño de la entrada crece, los algoritmos con complejidades como O(n²) y O(n³) se vuelven significativamente más lentos. Esto se debe al crecimiento polinómico de sus tiempos de ejecución. Por el contrario, los algoritmos con complejidades más eficientes, como O(1) y O(log n), mantienen tiempos de ejecución bajos incluso con entradas más grandes. Esto demuestra que, en situaciones prácticas, es crucial seleccionar algoritmos con menor orden de crecimiento para obtener un rendimiento óptimo (Goodrich & Tamassia, 2014).\nAdemás, los algoritmos con complejidades exponenciales y factoriales, como O(a^n) y O(n!), no son viables para tamaños de entrada relativamente pequeños, lo que limita su uso a problemas muy específicos y de pequeño tamaño (Van Emden, 2000).\n\n# Tiempos de ejecución para los tamaños de entrada especificados\nn_values_exec = [1, 10, 100, 1200]\n\n# Crear una tabla con los tiempos estimados\ntable = []\nfor n in n_values_exec:\n    row = [\n        n,\n        O1(np.array([n]))[0],\n        Ologn(np.array([n]))[0],\n        On(np.array([n]))[0],\n        Onlogn(np.array([n]))[0],\n        On2(np.array([n]))[0],\n        On3(np.array([n]))[0],\n        Oan(np.array([n]))[0],\n        Onfact(np.array([n]))[0],\n        Onn(np.array([n]))[0]\n    ]\n    table.append(row)\n\n# Mostrar la tabla\nimport pandas as pd\ndf = pd.DataFrame(table, columns=[\n    'n', 'O(1)', 'O(log n)', 'O(n)', 'O(n log n)', 'O(n²)', 'O(n³)', 'O(a^n)', 'O(n!)', 'O(n^n)'\n])\ndf\n\nC:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_3112\\1471029027.py:20: RuntimeWarning: overflow encountered in exp\n  return np.exp(n)\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n²)\nO(n³)\nO(a^n)\nO(n!)\nO(n^n)\n\n\n\n\n0\n1\n1\n0.000000\n1\n0.000000\n1\n1\n2.718282e+00\n1\n1\n\n\n1\n10\n1\n3.321928\n10\n33.219281\n100\n1000\n2.202647e+04\n3628800\n10000000000\n\n\n2\n100\n1\n6.643856\n100\n664.385619\n10000\n1000000\n2.688117e+43\n9332621544394415268169923885626670049071596826...\n0\n\n\n3\n1200\n1\n10.228819\n1200\n12274.582429\n1440000\n1728000000\ninf\n6350789086345676712402622313586536399392036192...\n0\n\n\n\n\n\n\n\n#CONCLUSIONES El análisis realizado ha demostrado cómo diferentes órdenes de crecimiento afectan los tiempos de ejecución de los algoritmos. A medida que analizamos los resultados, quedó claro que los algoritmos con complejidades como O(1) y O(log n) son mucho más eficientes y prácticos para entradas grandes. Mientras que los de orden O(n²) o O(n³) pueden ser viables para tamaños pequeños o medianos de entrada, los algoritmos con complejidades O(a^n), O(n!) o O(n^n) se vuelven rápidamente inalcanzables. Esto demuestra la importancia de elegir el algoritmo adecuado según el tamaño de los datos y el tipo de problema a resolver\n#REFERENCIAS • Goodrich, M. T., & Tamassia, R. (2014). Data structures and algorithms in Python. Wiley.\n• Sedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.). Addison-Wesley.\n• Van Emden, M. H. (2000). Algorithms and complexity: The theory and practice of computing. Oxford University Press."
  },
  {
    "objectID": "quarto/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "quarto/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "quarto/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto/Lib/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "href": "quarto/Lib/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "Listado de Mejoras",
    "section": "",
    "text": "Mejora: - Se integró el paquete {typst} para detección automática de:\n\nAusencia de acentos diacríticos\nUso inconsistente de mayúsculas\nPuntuación irregular\nLas ecuaciones ahora utilizan el entorno {math} con estilos diferenciados:"
  },
  {
    "objectID": "changelog.html#unidad-1-introducción-al-análisis-de-algoritmos",
    "href": "changelog.html#unidad-1-introducción-al-análisis-de-algoritmos",
    "title": "Listado de Mejoras",
    "section": "",
    "text": "Mejora: - Se integró el paquete {typst} para detección automática de:\n\nAusencia de acentos diacríticos\nUso inconsistente de mayúsculas\nPuntuación irregular\nLas ecuaciones ahora utilizan el entorno {math} con estilos diferenciados:"
  },
  {
    "objectID": "changelog.html#unidad-2-estructuras-de-datos",
    "href": "changelog.html#unidad-2-estructuras-de-datos",
    "title": "Listado de Mejoras",
    "section": "Unidad 2: Estructuras de datos",
    "text": "Unidad 2: Estructuras de datos\n\nMejora:Sin cambios mayores, solo se integra el desarrollo del proyecto dentro del mismo notebook y ya no en un archivo como word, esto para poder observar los resultados segun ls observaciones de la tarea."
  },
  {
    "objectID": "changelog.html#unidad-3-algoritmos-de-ordenamiento-por-comparación",
    "href": "changelog.html#unidad-3-algoritmos-de-ordenamiento-por-comparación",
    "title": "Listado de Mejoras",
    "section": "Unidad 3: Algoritmos de ordenamiento por comparación",
    "text": "Unidad 3: Algoritmos de ordenamiento por comparación\n\nMejora: Se mejora la estructura de acuerdo a lo señalado en la tarea, se refuerza el trabajo documental, y se modifica Bubbleshort para ser adaptativo, se optimiza el manejo de None, se eliminan las copias innecesarias de Mergesort, se corrige el pivote en quick sort y se reconstruye la estructura del skiplist basado en las referencias de las clases."
  },
  {
    "objectID": "changelog.html#unidad-4-algoritmos-de-búsqueda-por-comparación",
    "href": "changelog.html#unidad-4-algoritmos-de-búsqueda-por-comparación",
    "title": "Listado de Mejoras",
    "section": "Unidad 4: Algoritmos de búsqueda por comparación",
    "text": "Unidad 4: Algoritmos de búsqueda por comparación\n\nMejora: Se corrigen los algoritmos B1 y B2."
  },
  {
    "objectID": "changelog.html#unidad-5-algoritmos-de-intersección-y-unión-de-conjuntos-en-el-modelo-de-comparación",
    "href": "changelog.html#unidad-5-algoritmos-de-intersección-y-unión-de-conjuntos-en-el-modelo-de-comparación",
    "title": "Listado de Mejoras",
    "section": "Unidad 5: Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "text": "Unidad 5: Algoritmos de intersección y unión de conjuntos en el modelo de comparación\n\nMejora: Sin cambios"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mí",
    "section": "",
    "text": "Ingeniero en Mecatrónica\nEstudiante de Lic. en Economía en la UNAM\nEstudiante de Maestría en Ciencia de Datos e Información en INFOTEC\n\n\nEspecialista en web scraping, con experiencia en valuación de activos industriales. Mi necesidad de datos precisos para valuación me llevó a dominar técnicas de extracción y análisis avanzado, desarrollando modelos predictivos de depreciación.\n\n\n\n\n  LinkedIn \n  GitHub \n  Portafolio Personal \n\n\n\n\n\nWeb Scraping estatico y dinamico\nAplicacion de Ingenieria inversa para consultas a APIs\nOrquestadores de trabajo mediante Airflow\nProcesos ETL"
  },
  {
    "objectID": "about.html#perfil-profesional",
    "href": "about.html#perfil-profesional",
    "title": "Sobre Mí",
    "section": "",
    "text": "Especialista en web scraping, con experiencia en valuación de activos industriales. Mi necesidad de datos precisos para valuación me llevó a dominar técnicas de extracción y análisis avanzado, desarrollando modelos predictivos de depreciación."
  },
  {
    "objectID": "about.html#contacto",
    "href": "about.html#contacto",
    "title": "Sobre Mí",
    "section": "",
    "text": "LinkedIn \n  GitHub \n  Portafolio Personal"
  },
  {
    "objectID": "about.html#habilidades",
    "href": "about.html#habilidades",
    "title": "Sobre Mí",
    "section": "",
    "text": "Web Scraping estatico y dinamico\nAplicacion de Ingenieria inversa para consultas a APIs\nOrquestadores de trabajo mediante Airflow\nProcesos ETL"
  },
  {
    "objectID": "quarto/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "quarto/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "quarto/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "quarto/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "quarto/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto/Lib/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "quarto/Lib/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "unidad2.html",
    "href": "unidad2.html",
    "title": "Arif Narváez - Analisis de Algoritmos",
    "section": "",
    "text": "#INTRODUCCION\nLas operaciones con matrices son fundamentales en computación científica, ingeniería y ciencia de datos. En este reporte, exploramos dos algoritmos clave: * Multiplicación de matrices. * Eliminación Gaussiana/Gauss-Jordan.\n** Objetivo principal ** Comparar el desempeño de estos algoritmos en términos de tiempo de ejecución y conteo de operaciones para matrices aleatorias de tamaño n×n, con valores de n=100,300,1000. Para ello, analizaremos las propiedades matemáticas de las matrices, su impacto en el rendimiento computacional y consideraciones de estabilidad numérica, que es un problema relevante debido a la representación finita de los números en las computadoras.\n#FUNDAMENTOS TEORICOS Propiedades de las Matrices y Rango Las matrices son estructuras matemáticas fundamentales en la solución de sistemas de ecuaciones lineales y otros problemas computacionales. En particular, el rango de una matriz juega un papel crucial en la resolución de sistemas. De acuerdo con la teoría de matrices, una matriz cuadrada n×n tiene rango completo si su rango es n. Esto implica que sus filas (o columnas) son linealmente independientes y que la matriz es invertible. Además, el acceso eficiente a memoria juega un papel crucial en el rendimiento de los algoritmos de matrices, ya que la disposición de los datos en la memoria afecta el tiempo de ejecución (Golub & Van Loan, 2013)\nMultiplicación de Matrices La multiplicación de dos matrices A y B de tamaño n×n se define como:​\n\n\n\nCaptura.JPG\n\n\nEl algoritmo tradicional tiene una complejidad de O(n³) debido a los tres bucles anidados. Según la segunda edición de “Introduction to Algorithms” de The Massachusetts Institute of Technology, existen métodos avanzados como el algoritmo de Strassen, que reduce la complejidad a O (n^ {2.81}), aunque no se implementó en este experimento.\nEliminación Gaussiana y Gauss-Jordan\nLa eliminación gaussiana es un método estándar para resolver sistemas de ecuaciones lineales, transformando la matriz aumentada en una forma escalonada superior mediante eliminación de filas. Su extensión, Gauss-Jordan, transforma la matriz en la identidad para hallar directamente la inversa. Este proceso sigue los siguientes pasos: * Pivoteo: Selección del elemento pivote. * Eliminación hacia adelante: Conversión de coeficientes en ceros. * Sustitución hacia atrás: Resolución del sistema de ecuaciones.\nEl número de operaciones sigue un crecimiento cúbico, similar a la multiplicación de matrices, con una complejidad O(n³).\n#IMPLEMENTACIÓN EN PYTHON\nSe desarrollaron dos versiones:\n\nMétodos tradicionales con bucles (para el conteo de operaciones).\nMétodos optimizados con NumPy (para mejorar el rendimiento).\n\nMultiplicación de Matrices\nSe implementó la multiplicación con tres bucles anidados y se contó el número de operaciones de suma y multiplicación.\nEliminación Gaussiana\nSe utilizó pivoteo parcial y reducción de filas para obtener una matriz triangular superior. Se contabilizaron las operaciones de suma y multiplicación necesarias. Para ambas operaciones, se midió el tiempo de ejecución usando time.time()\nComenzamos importando las librerías a utilizar\n\nimport numpy as np\nimport time\n\nAplicación de una función “Multiplicacion_matriz” para realizar la multiplicación de matrices aplicando un ciclo doble for para recorrer filas y columnas segunda la dimensión especificada por n y guardando en la variable sum_val la iteración de k en A[i,k] * B[k,j]\n\n#multiplicación de matrices con conteo de operaciones\ndef Multiplicacion_matriz(A, B):\n    n = A.shape[0]\n    C = np.zeros((n, n))\n    operaciones = {\"MULTIPLICACIONES\": 0, \"SUMAS\": 0}\n\n    for i in range(n):\n        for j in range(n):\n            sum_val = 0\n            for k in range(n):\n                sum_val += A[i, k] * B[k, j]\n                operaciones[\"MULTIPLICACIONES\"] += 1  #conteo de multiplicación\n                operaciones[\"SUMAS\"] += 1 if k &gt; 0 else 0  #conteo de sumas\n\n            C[i, j] = sum_val\n\n    return C, operaciones\n\nCreamos la función “Gauss_jordan” donde se realiza la eliminación de Gauss-Jordan para resolver un sistema de ecuaciones lineales representado por una matriz aumentada. Primero, convierte la matriz a tipo float para mayor precisión. Luego utiliza un bucle for principal para seleccionar cada pivote (elemento diagonal) y un bucle for secundario para eliminar los elementos debajo del pivote, calculando un factor de multiplicación y actualizando las filas correspondientes. Asi mismo se cuentan las multiplicaciones y sumas realizadas para finalmente devolver la matriz transformada (en forma escalonada) y el conteo de operaciones aritméticas.\n\n#eliminación gaussiana con conteo de operaciones\ndef Gauss_jordan(A):\n    n = A.shape[0]\n    A = A.astype(float)\n    operaciones = {\"MULTIPLICACIONES\": 0, \"SUMAS\": 0}\n\n    for i in range(n):\n        for j in range(i+1, n):\n            if A[i, i] == 0:\n                continue\n            factor = A[j, i] / A[i, i]\n            operaciones[\"MULTIPLICACIONES\"] += 1\n\n            for k in range(i, n):\n                A[j, k] -= factor * A[i, k]\n                operaciones[\"MULTIPLICACIONES\"] += 1\n                operaciones[\"SUMAS\"] += 1\n\n    return A, operaciones\n\nPosteriormente creamos la funcion “referencia” en donde buscamos comparar el tiempo de ejecución y el número de operaciones realizadas en dos procesos clave: la multiplicación de matrices y la eliminación Gaussiana. Primero, genera dos matrices aleatorias A y Bde tamaño n×n. Luego, mide el tiempo y cuenta las operaciones para multiplicar A y B utilizando la función “Multiplicacion_matriz”. Después, hace lo mismo para aplicar la eliminación Gaussiana a la matriz A usando la función “Gauss_jordan”. Finalmente, devuelve un diccionario con el tamaño de la muestra n, los tiempos de ejecución y los contadores de operaciones para ambos procesos.\n\n#tiempos y operaciones\ndef referencia(n):\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    #multiplicación de matrices\n    start = time.time()\n    _, ops_mult = Multiplicacion_matriz(A, B)\n    time_mult = time.time() - start\n\n    #eliminación Gaussiana\n    start = time.time()\n    _, ops_gauss = Gauss_jordan(A)\n    time_gauss = time.time() - start\n\n    return {\"TAMAÑO DE MUESTRA\": n, \"Tiempo Multiplicacion\": time_mult, \"Contador Multiplicacion\": ops_mult,\n            \"Tiempo Gauss jordan\": time_gauss, \"Contador Gauss jordan\": ops_gauss}\n\nFinalmente aplicamos la función referencia para cada una de las dimensiones estipuladas en el ejercicio.\n\n# definimos las dimensiones solicitadas para n=100, 300, 1000\ndimensiones = [100, 300, 1000]\nresultados = [referencia(n) for n in dimensiones]\nresultados\n\n[{'TAMAÑO DE MUESTRA': 100,\n  'Tiempo Multiplicacion': 2.6025776863098145,\n  'Contador Multiplicacion': {'MULTIPLICACIONES': 1000000, 'SUMAS': 990000},\n  'Tiempo Gauss jordan': 1.142932653427124,\n  'Contador Gauss jordan': {'MULTIPLICACIONES': 338250, 'SUMAS': 333300}},\n {'TAMAÑO DE MUESTRA': 300,\n  'Tiempo Multiplicacion': 48.76987051963806,\n  'Contador Multiplicacion': {'MULTIPLICACIONES': 27000000, 'SUMAS': 26910000},\n  'Tiempo Gauss jordan': 20.549858808517456,\n  'Contador Gauss jordan': {'MULTIPLICACIONES': 9044750, 'SUMAS': 8999900}},\n {'TAMAÑO DE MUESTRA': 1000,\n  'Tiempo Multiplicacion': 2063.898496389389,\n  'Contador Multiplicacion': {'MULTIPLICACIONES': 1000000000,\n   'SUMAS': 999000000},\n  'Tiempo Gauss jordan': 726.8457300662994,\n  'Contador Gauss jordan': {'MULTIPLICACIONES': 333832500,\n   'SUMAS': 333333000}}]\n\n\nLa función Multiplicacion_matriz_np realiza la multiplicación de matrices de manera optimizada utilizando la función np.dot de NumPy, que está altamente optimizada para operaciones matriciales. Primero, toma como entrada dos matrices A y B. Luego, mide el tiempo de inicio y ejecuta la multiplicación de matrices usando np.dot(A, B), que calcula el producto matricial de manera eficiente. Después, calcula el tiempo transcurrido restando el tiempo inicial del tiempo final. Finalmente, la función devuelve la matriz resultante C y el tiempo que tomó realizar la multiplicación.\n\n# funcion optimizada para multiplicación de matrices usando NumPy\ndef Multiplicacion_matriz_np(A, B):\n    inicio = time.time()\n    C = np.dot(A, B)  # este es el cambio donde realizamos la multiplicación optimizada con NumPy\n    tiempo = time.time() - inicio\n    return C, tiempo\n\nLa función “Gauss_jordan_np” realiza la eliminación Gaussiana de manera optimizada, evitando bucles innecesarios y aprovechando operaciones vectorizadas de NumPy. Primero, convierte la matriz A a tipo float para garantizar precisión en los cálculos. Luego, itera sobre cada fila para seleccionar el pivote A[i,i]]. Si el pivote es cero, se salta esa fila. Para cada fila debajo del pivote, calcula un factor de eliminación y actualiza la fila completa desde la columna ii en adelante usando una operación vectorizada (A[j,i:]−=factor∗A[i,i:]), lo que elimina la necesidad de un bucle adicional sobre las columnas. Finalmente, mide el tiempo de ejecución y devuelve la matriz transformada (en forma escalonada) y el tiempo que tomó realizar el proceso.\n\n# funcion optimizada para eliminación gaussiana sin bucles innecesarios\ndef Gauss_jordan_np(A):\n    inicio = time.time()\n    A = A.astype(float)\n    n = A.shape[0]\n    for i in range(n):\n        if A[i, i] == 0:\n            continue\n        for j in range(i+1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n    tiempo = time.time() - inicio\n    return A, tiempo\n\nDe igual forma que en la implementación del método tradicional se busca comparar el tiempo de ejecución y el número de operaciones realizadas en dos procesos clave: la multiplicación de matrices y la eliminación Gaussiana pero con las funciones optimizadas con numpy.\n\n# funcion optimizada para benchmark\ndef referencia_np(n):\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # multiplicación de matrices optimizada\n    _, time_mult = Multiplicacion_matriz_np(A, B)\n\n    # eliminación Gaussiana optimizada\n    _, time_gauss = Gauss_jordan_np(A)\n\n    return {\"TAMAÑO DE MUESTRA\": n, \"Tiempo multiplicacion\": time_mult, \"Tiempo Gauss-Jordan\": time_gauss}\n\nFinalmente aplicamos la función referencia para cada una de las dimensione estipuladas en el ejercicio.\n\ndimensiones_op = [100, 300, 1000]\nresultados_np = [referencia_np(n) for n in dimensiones_op]\nresultados_np\n\n[{'TAMAÑO DE MUESTRA': 100,\n  'Tiempo multiplicacion': 0.1938190460205078,\n  'Tiempo Gauss-Jordan': 0.12288737297058105},\n {'TAMAÑO DE MUESTRA': 300,\n  'Tiempo multiplicacion': 0.0029985904693603516,\n  'Tiempo Gauss-Jordan': 0.710338830947876},\n {'TAMAÑO DE MUESTRA': 1000,\n  'Tiempo multiplicacion': 0.09091663360595703,\n  'Tiempo Gauss-Jordan': 8.559103965759277}]\n\n\n#MATRICES DISPERSAS\n\nimport scipy.sparse as sp\n\n\n# realizamos el ejercicio mas significativo con n=1000\nn = 1000  # Tamaño de la matriz (n x n)\ndensidad = 0.01  # especificamos un porcentaje de elementos no nulos en la matriz dispersa\n\n# se crea una matriz dispersa aleatoria\nA_dispersa = sp.random(n, n, density=densidad, format='csr')  # las siglas csr hacen referencia a Compressed Sparse Row\nB_dispersa = sp.random(n, n, density=densidad, format='csr')\n\n# multiplicación de matrices dispersas\ndef multiplicacion_dispersa(A, B):\n    inicio = time.time()\n    C = A.dot(B)  # multiplicación optimizada para matrices dispersas\n    tiempo = time.time() - inicio\n    return C, tiempo\n\n#eliminación Gaussiana en matrices dispersas\ndef gauss_jordan_dispersa(A):\n    inicio = time.time()\n    A = A.astype(float)\n    n = A.shape[0]\n    for i in range(n):\n        if A[i, i] == 0:\n            continue\n        for j in range(i+1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n    tiempo = time.time() - inicio\n    return A, tiempo\n\n\nC_dispersa, tiempo_dispersa = multiplicacion_dispersa(A_dispersa, B_dispersa)\nprint(f\"Tiempo multiplicación dispersa: {tiempo_dispersa:.4f} segundos\")\n\nA_dispersa_gauss, tiempo_gauss_dispersa = gauss_jordan_dispersa(A_dispersa.toarray())  # convertir a densa para el cálculo\nprint(f\"Tiempo eliminación Gaussiana (dispersa, convertida a densa): {tiempo_gauss_dispersa:.4f} segundos\")\n\nTiempo multiplicación dispersa: 0.0050 segundos\nTiempo eliminación Gaussiana (dispersa, convertida a densa): 0.3317 segundos\n\n\n#ANALISIS DE LOS RESULTADOS * Considerando únicamente n=1000 podemos observar que en la multiplicación de matrices como la eliminación gauss jordan y la multiplicación con NumPy es casi 10,000 veces más rápida que el método tradicional. * La eliminación gaussiana tiene un crecimiento cúbico en tiempo debido al gran número de operaciones requeridas.\nImpacto del Acceso a Memoria\nEl acceso eficiente a memoria es clave en operaciones matriciales: * NumPy optimiza la multiplicación aprovechando acceso contiguo a memoria y vectorización, describiendo así la ausencia de bucles, indexaciones, etc. explícitos en el código (NumPy Org, 2025). * La eliminación gaussiana es más costosa, ya que requiere múltiples modificaciones en la estructura de la matriz, lo que genera saltos de memoria y reduce la eficiencia del caché.\n#CONCLUSIONES El impacto de acceder a elementos contiguos en memoria en una matriz es significativo en términos de eficiencia y rendimiento (NumPy Org, 2025). Cuando los datos están almacenados de manera contigua (como en los arreglos de NumPy), el acceso a la memoria es más rápido debido a la localidad espacial, lo que permite un mejor uso de la caché del procesador y reduce los tiempos de acceso. Esto es especialmente importante en operaciones matriciales, donde se accede repetidamente a grandes bloques de datos.\nPor otro lado, si se utilizan matrices dispersas (donde la mayoría de los elementos son cero), el acceso a los datos no contiguos y la gestión de la memoria pueden volverse más costosos en términos de tiempo y espacio. Si se trabaja con matrices dispersas, se deben utilizar estructuras de datos especializadas (SciPy Org, 2025) para almacenar solo los elementos no nulos y sus posiciones. Esto reduce el uso de memoria, pero introduce costos adicionales en las operaciones, como:\n\nMayor complejidad algorítmica: Las operaciones básicas (como multiplicación o eliminación Gaussiana) requieren algoritmos específicos para manejar la dispersión.\nSobrecarga de índices: Se necesita almacenar información adicional (como filas, columnas y valores no nulos), lo que aumenta el overhead.\nAcceso no contiguo: El acceso a elementos dispersos puede ser más lento debido a la falta de localidad espacial en la memoria."
  },
  {
    "objectID": "unidad4.html",
    "href": "unidad4.html",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "",
    "text": "Docente: Dr. Eric Sadit Téllez Avila\nAlumno: Arif Narvaez de la O"
  },
  {
    "objectID": "unidad4.html#introducción",
    "href": "unidad4.html#introducción",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "1. Introducción",
    "text": "1. Introducción\nEste documento describe la implementación y evaluación de diversos algoritmos de búsqueda aplicados a un conjunto de datos ordenado. Se han considerado cinco métodos de búsqueda distintos:\n\nBúsqueda binaria acotada\nBúsqueda secuencial (B0)\nBúsqueda no acotada B1\nBúsqueda no acotada B2\nBúsqueda utilizando la estructura SkipList\n\nEl objetivo es medir la eficiencia de cada algoritmo en términos de número de comparaciones y tiempo de ejecución utilizando varios archivos de consulta. Los resultados permiten analizar cuál de estas estrategias es más óptima en el contexto de datos ordenados.\nDe acuerdo con Cormen et al. (2009), la búsqueda binaria es óptima en listas ordenadas, mientras que Knuth (1998) destaca que la búsqueda secuencial es ineficiente en grandes volúmenes de datos. La estructura SkipList, introducida por Pugh (1990), se considera una alternativa eficiente que permite búsquedas rápidas mediante listas enlazadas con múltiples niveles."
  },
  {
    "objectID": "unidad4.html#librerias-a-utilizar",
    "href": "unidad4.html#librerias-a-utilizar",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "2. Librerias a utilizar",
    "text": "2. Librerias a utilizar\n\nimport json\nimport time\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom bisect import bisect_left\nfrom sortedcontainers import SortedList\nimport random"
  },
  {
    "objectID": "unidad4.html#carga-de-datos",
    "href": "unidad4.html#carga-de-datos",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "3.Carga de datos",
    "text": "3.Carga de datos\nEsta Secc¿cion de codigo abre y carga los datos desde un archivo JSON, extrayendo las claves y ordenándolas para facilitar las búsquedas.\n\nruta_datos = \"./archivos/p-032_ordenado.json\"\nruta_consultas = [\n    f\"./archivos/consultas-{i}-listas-posteo.json\"\n    for i in range(1, 5)\n]\n\nwith open(ruta_datos, 'r') as f:\n    datos = json.load(f)\n\ndatos_ordenados = sorted(datos.keys())\n\nVariables y su función * ruta_datos: Almacena la ruta del archivo JSON que contiene la lista ordenada de claves principales.\n\nruta_consultas: Lista de rutas a los archivos JSON que contienen las consultas a realizar.\ndatos: Diccionario cargado desde el archivo JSON, con las claves como índices.\ndatos_ordenados: Lista de claves extraídas y ordenadas para su uso en los algoritmos de búsqueda."
  },
  {
    "objectID": "unidad4.html#implementacion-de-los-algoritmos-de-busqueda",
    "href": "unidad4.html#implementacion-de-los-algoritmos-de-busqueda",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "4. Implementacion de los algoritmos de busqueda",
    "text": "4. Implementacion de los algoritmos de busqueda\nBUSQUEDA BINARIA\nEste algoritmo tiene una complejidad de O(log n) y es eficiente para listas ordenadas (Cormen et al., 2009).\n\nFunción: busqueda_binaria(lista, clave)\nVariables clave:\n\nizquierda y derecha: Definen los límites del área de búsqueda.\nmedio: Calcula el punto intermedio para dividir la lista.\ncomparaciones: Cuenta el número de comparaciones realizadas.\n\n\n\ndef busqueda_binaria(A, x):\n    \"\"\"Búsqueda binaria clásica para encontrar posición de inserción.\"\"\"\n    comparaciones = 0\n    sp, ep = 0, len(A) - 1\n\n    while sp &lt; ep:\n        comparaciones += 1\n        mid = (sp + ep) // 2\n        if x &lt;= A[mid]:\n            ep = mid\n        else:\n            sp = mid + 1\n\n    comparaciones += 1\n    return (sp if x &lt;= A[sp] else sp + 1), comparaciones\n\nBUSQUEDA SECUENCIAL\nEste método tiene una complejidad de O(n), lo que lo hace ineficiente en grandes volúmenes de datos (Knuth, 1998).\nAdemas, revisa cada elemento de la lista, de izquierda a derecha, hasta encontrar el valor buscado o llegar al final.\n\nFunción: busqueda_secuencial(lista, clave)\nVariables clave:\n\ncomparaciones: Cuenta cuántas veces se compara la clave con elementos de la list\n\n\n\ndef busqueda_secuencial(A, x):\n    \"\"\"Búsqueda secuencial para encontrar posición de inserción.\"\"\"\n    comparaciones = 0\n    for i, elemento in enumerate(A):\n        comparaciones += 1\n        if x &lt;= elemento:\n            return i, comparaciones\n    return len(A), comparaciones\n\nBUSQUEDA NO ACOTADA B1\nEste método selecciona posiciones aleatorias dentro de la lista y compara el valor en dichas posiciones con la clave buscada. Si después de un número fijo de intentos no encuentra el elemento, la búsqueda se considera fallida. Es menos eficiente en listas ordenadas, ya que no aprovecha la estructura de los datos. Su eficiencia depende de la distribución de los datos y la suerte en los intentos aleatorios.\n\nFunción: busqueda_no_acotada_b1(lista, clave)\n\nCaracterísticas:\nSe eligen 10 posiciones aleatorias para buscar la clave.\n\ndef busqueda_no_acotada_B1(A, x):\n    \"\"\"\n    Algoritmo B1: Búsqueda no acotada con doblado simple (doubling search/galloping)\n    Implementación según Bentley y Yao (1976)\n    \"\"\"\n    comparaciones = 0\n    n = len(A)\n\n    if n == 0:\n        return 0, 0\n\n    # Fase 1: Determinación del rango con doblado\n    i = 1\n    p = 0\n    while p + i &lt; n and A[p + i] &lt; x:\n        comparaciones += 1\n        p += i\n        i *= 2\n\n    # Fase 2: Búsqueda binaria en el rango encontrado\n    sp = p\n    ep = min(p + i, n - 1)\n\n    while sp &lt; ep:\n        comparaciones += 1\n        mid = (sp + ep) // 2\n        if x &lt;= A[mid]:\n            ep = mid\n        else:\n            sp = mid + 1\n\n    comparaciones += 1\n    return (sp if x &lt;= A[sp] else sp + 1), comparaciones\n\nBUSQUEDA NO ACOTADA B2\nEsta variante combina la búsqueda secuencial con saltos aleatorios en la lista, lo que introduce cierta aleatoriedad en la exploración. Si bien intenta mejorar la búsqueda secuencial, no logra un rendimiento estable debido a la naturaleza aleatoria de los saltos.\n\nFunción: busqueda_no_acotada_b2(lista, clave)\n\nCaracterísticas:\nMezcla búsqueda secuencial con saltos aleatorios.\n\ndef busqueda_no_acotada_B2(A, x):\n    \"\"\"\n    Algoritmo B2: Búsqueda no acotada con doblado doble (doubling-doubling search)\n    Implementación según Bentley y Yao (1976)\n    \"\"\"\n    comparaciones = 0\n    n = len(A)\n\n    if n == 0:\n        return 0, 0\n\n    # Fase 1: Determinación del rango con doblado doble\n    i = 1\n    j = 1\n    p = 0\n    while p + i &lt; n and A[p + i] &lt; x:\n        comparaciones += 1\n        p += i\n        i = j * j\n        j += 1\n\n    # Fase 2: Aplicar B1 en el rango encontrado\n    sp = p\n    ep = min(p + i, n - 1)\n\n    # Implementación de B1 dentro del rango\n    k = 1\n    q = sp\n    while q + k &lt; ep and A[q + k] &lt; x:\n        comparaciones += 1\n        q += k\n        k *= 2\n\n    # Fase 3: Búsqueda binaria en el subrango final\n    left = q\n    right = min(q + k, ep)\n\n    while left &lt; right:\n        comparaciones += 1\n        mid = (left + right) // 2\n        if x &lt;= A[mid]:\n            right = mid\n        else:\n            left = mid + 1\n\n    comparaciones += 1\n    return (left if x &lt;= A[left] else left + 1), comparaciones\n\nSKIP LIST\nSkipList es una estructura de datos que permite búsquedas eficientes mediante listas enlazadas con múltiples niveles de acceso. Su rendimiento es cercano al de la búsqueda binaria, con una complejidad promedio de O(logn) (Pugh, 1990).\n\nFunción: SkipList.buscar(clave)\nEstructura: Utiliza SortedList para optimizar la búsqueda.\n\n\nclass SkipList:\n    \"\"\"Implementación mejorada de SkipList para búsqueda eficiente.\"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n        self.levels = []\n        self.levels.append([])  # Nivel 0 contiene todos los elementos\n\n    def insertar(self, valor):\n        # Insertar en todos los niveles necesarios\n        self.levels[0].append(valor)\n        self.levels[0].sort()  # Mantener ordenado\n\n        # Decidir si promocionar el elemento a niveles superiores\n        current_level = 0\n        while random.random() &lt; self.p:\n            current_level += 1\n            if current_level &gt;= len(self.levels):\n                self.levels.append([])\n\n            # Insertar en el nivel superior\n            self.levels[current_level].append(valor)\n            self.levels[current_level].sort()\n\n    def buscar(self, clave):\n        comparaciones = 0\n        # Comenzar desde el nivel más alto\n        level = len(self.levels) - 1\n        pos = -1\n\n        while level &gt;= 0:\n            current_list = self.levels[level]\n            idx = bisect_left(current_list, clave)\n\n            if idx &lt; len(current_list) and current_list[idx] == clave:\n                comparaciones += 1\n                return idx, comparaciones\n\n            comparaciones += 1\n            if idx &gt; 0 and level &gt; 0:\n                # Buscar en el rango correspondiente en el nivel inferior\n                lower_val = current_list[idx-1]\n                lower_idx = bisect_left(self.levels[level-1], lower_val)\n                pos = lower_idx\n            elif level == 0:\n                pos = idx\n\n            level -= 1\n\n        return pos, comparaciones\n\n# Preparar estructuras de datos\nskiplist = SkipList()\nfor elemento in datos_ordenados:\n    skiplist.insertar(elemento)"
  },
  {
    "objectID": "unidad4.html#resultados",
    "href": "unidad4.html#resultados",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "5. Resultados",
    "text": "5. Resultados\nEsta función, mostrar_resultados(), tiene como objetivo convertir un diccionario de resultados (resultados) en un diccionario de DataFrames de pandas, donde cada DataFrame corresponde a una consulta específica.\ndf_consultas = {}:\n\nSe inicializa un diccionario vacío que almacenará los DataFrames generados.\nLas claves serán strings como “df_consulta1”, “df_consulta2”, etc.\nLos valores serán DataFrames de pandas.\n\n\n# Función para ejecutar pruebas\ndef ejecutar_pruebas(consulta_file):\n    with open(consulta_file, 'r') as f:\n        consultas = json.load(f)\n\n    metodos = {\n        \"binaria\": busqueda_binaria,\n        \"secuencial\": busqueda_secuencial,\n        \"B1_doubling\": busqueda_no_acotada_B1,\n        \"B2_doubling-doubling\": busqueda_no_acotada_B2,\n        \"skiplist\": skiplist.buscar\n    }\n\n    resultados = {}\n\n    for nombre, metodo in metodos.items():\n        total_comparaciones = 0\n        tiempo_inicio = time.time()\n\n        for consulta in consultas:\n            clave = str(consulta)\n            if nombre == \"skiplist\":\n                _, comparaciones = metodo(clave)\n            else:\n                _, comparaciones = metodo(datos_ordenados, clave)\n            total_comparaciones += comparaciones\n\n        tiempo_total = time.time() - tiempo_inicio\n        resultados[nombre] = {\n            \"comparaciones\": total_comparaciones,\n            \"tiempo\": tiempo_total,\n            \"comparaciones_promedio\": total_comparaciones / len(consultas)\n        }\n\n    return resultados\n\n# Ejecutar pruebas para cada archivo de consultas\nresultados_totales = {}\nfor consulta_file in ruta_consultas:\n    resultados_totales[consulta_file] = ejecutar_pruebas(consulta_file)\n\n# Mostrar resultados\ndef mostrar_resultados():\n    dfs = []\n    for i, (consulta_file, datos) in enumerate(resultados_totales.items(), 1):\n        df = pd.DataFrame(datos).T\n        df['Archivo'] = f'Consulta {i}'\n        dfs.append(df)\n\n    # Concatenar todos los DataFrames\n    df_final = pd.concat(dfs)\n\n    # Reorganizar los datos para mejor visualización\n    # Primero resetear el índice para tener los nombres de métodos como columna\n    df_final = df_final.reset_index().rename(columns={'index': 'Metodo'})\n\n    # Usar melt para tener una estructura más adecuada\n    df_melted = pd.melt(df_final, id_vars=['Archivo', 'Metodo'],\n                        value_vars=['comparaciones', 'tiempo', 'comparaciones_promedio'],\n                        var_name='Metrica', value_name='Valor')\n\n    # Pivotar para tener métodos como columnas\n    df_pivoted = df_melted.pivot_table(index=['Archivo', 'Metrica'],\n                                      columns='Metodo', values='Valor')\n\n    return df_pivoted.reset_index()\n\ndf_consultas = mostrar_resultados()\ndf_consultas\n\n\n\n\n\n\n\nMetodo\nArchivo\nMetrica\nB1_doubling\nB2_doubling-doubling\nbinaria\nsecuencial\nskiplist\n\n\n\n\n0\nConsulta 1\ncomparaciones\n50000.000000\n60000.000000\n70000.000000\n70000.000000\n60000.000000\n\n\n1\nConsulta 1\ncomparaciones_promedio\n5.000000\n6.000000\n7.000000\n7.000000\n6.000000\n\n\n2\nConsulta 1\ntiempo\n0.046958\n0.043958\n0.044957\n0.027974\n0.121886\n\n\n3\nConsulta 2\ncomparaciones\n50000.000000\n60000.000000\n70000.000000\n70000.000000\n60000.000000\n\n\n4\nConsulta 2\ncomparaciones_promedio\n5.000000\n6.000000\n7.000000\n7.000000\n6.000000\n\n\n5\nConsulta 2\ntiempo\n0.021980\n0.028974\n0.047955\n0.016983\n0.070936\n\n\n6\nConsulta 3\ncomparaciones\n50000.000000\n60000.000000\n70000.000000\n70000.000000\n60000.000000\n\n\n7\nConsulta 3\ncomparaciones_promedio\n5.000000\n6.000000\n7.000000\n7.000000\n6.000000\n\n\n8\nConsulta 3\ntiempo\n0.041963\n0.102904\n0.020980\n0.022977\n0.178831\n\n\n9\nConsulta 4\ncomparaciones\n50000.000000\n60000.000000\n70000.000000\n70000.000000\n60000.000000\n\n\n10\nConsulta 4\ncomparaciones_promedio\n5.000000\n6.000000\n7.000000\n7.000000\n6.000000\n\n\n11\nConsulta 4\ntiempo\n0.048954\n0.063941\n0.043961\n0.032968\n0.095910\n\n\n\n\n\n\n\n\nresultados.items():\n\nAsumimos que resultados es un diccionario externo con una estructura como:\n.items() devuelve pares (clave, valor) del diccionario.\n\nenumerate(…, start=1):\n\nNumera las consultas comenzando desde 1 (en lugar de 0).\ni toma valores 1, 2, 3, … para generar nombres como df_consulta1.\nconsulta_file es la clave del diccionario (ej: “consulta1”).\ndatos es el valor asociado (ej: {“método1”: {“comparaciones”: 10, “tiempo”: 0.5}, …}).\n\ndf_nombre = f”df_consulta{i}“:\n\nGenera un nombre dinámico para el DataFrame (ej: “df_consulta1”).\n\npd.DataFrame(datos).T:\n\ndatos es un diccionario anidado (ej: {“método1”: {“comparaciones”: 10, “tiempo”: 0.5}}).\npd.DataFrame(datos) lo convierte en un DataFrame donde:\n\nFilas: Claves del diccionario interno (“comparaciones”, “tiempo”).\nColumnas: Métodos de búsqueda (“método1”, “método2”, etc.).\n\n\n.T transpone el DataFrame para que:\n\nFilas: Métodos de búsqueda.\nColumnas: Métricas (“comparaciones”, “tiempo”).\n\n\n\nprint(f\"LISTA DE CONSULTAS 1\")\ngrouped = df_consultas.groupby('Archivo')\ndf_consulta1 = grouped.get_group('Consulta 1').copy()\n\nLISTA DE CONSULTAS 1\n\n\n\nprint(f\"LISTA DE CONSULTAS 2\")\ngrouped = df_consultas.groupby('Archivo')\ndf_consulta2 = grouped.get_group('Consulta 2').copy()\n\nLISTA DE CONSULTAS 2\n\n\n\nprint(f\"LISTA DE CONSULTAS 3\")\ngrouped = df_consultas.groupby('Archivo')\ndf_consulta3 = grouped.get_group('Consulta 3').copy()\n\nLISTA DE CONSULTAS 3\n\n\n\nprint(f\"LISTA DE CONSULTAS 4\")\ngrouped = df_consultas.groupby('Archivo')\ndf_consulta4 = grouped.get_group('Consulta 4').copy()\n\nLISTA DE CONSULTAS 4\n\n\n\nplt.style.use('ggplot')\nplt.figure(figsize=(15, 10))\n\nmetricas = df_consultas['Metrica'].unique()\nmetodos = [col for col in df_consultas.columns if col not in ['Archivo', 'Metrica']]\nconsultas = df_consultas['Archivo'].unique()\n\n# Crear subgráficos para cada métrica\nfig, axes = plt.subplots(len(metricas), 1, figsize=(14, 12))\n\n# Colores para cada método\ncolors = plt.cm.get_cmap('tab10', len(metodos))\n\nfor i, metrica in enumerate(metricas):\n    ax = axes[i] if len(metricas) &gt; 1 else axes\n    df_metrica = df_consultas[df_consultas['Metrica'] == metrica]\n\n    # Ancho de las barras\n    width = 0.15\n    x = np.arange(len(consultas))\n\n    for j, metodo in enumerate(metodos):\n        valores = df_metrica[metodo].values\n        ax.bar(x + j*width, valores, width, label=metodo, color=colors(j))\n\n\n    ax.set_title(f'Métrica: {metrica.capitalize()}', fontsize=12)\n    ax.set_xticks(x + width*(len(metodos)-1)/2)\n    ax.set_xticklabels(consultas)\n    ax.set_ylabel(metrica.capitalize())\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n\n    if i == len(metricas)-1:\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()\nplt.suptitle('Comparación de Métodos de Búsqueda por Consulta', y=1.02, fontsize=14)\nplt.show()\n\nC:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_4464\\607428116.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap('tab10', len(metodos))\n\n\n&lt;Figure size 1440x960 with 0 Axes&gt;"
  },
  {
    "objectID": "unidad4.html#conclusiones",
    "href": "unidad4.html#conclusiones",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "6. Conclusiones",
    "text": "6. Conclusiones\nLos resultados muestran que la búsqueda binaria y la estructura de datos SkipList son significativamente más eficientes en términos de número de comparaciones y tiempo de ejecución.\nLa búsqueda secuencial y las variantes no acotadas presentan un alto número de comparaciones y tiempos de respuesta más largos, lo que las hace menos recomendables para listas ordenadas. En particular, la búsqueda no acotada B2, que combina exploración secuencial con saltos aleatorios, mostró el peor desempeño en comparación con los demás algoritmos.\nPor otro lado, la implementación de estructuras avanzadas como SkipList puede optimizar la búsqueda en grandes volúmenes de datos. Su rendimiento, en algunos casos, es incluso mejor que la búsqueda binaria debido a su estructura de múltiples niveles de acceso."
  },
  {
    "objectID": "unidad4.html#referencias",
    "href": "unidad4.html#referencias",
    "title": "4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación.",
    "section": "7. Referencias",
    "text": "7. Referencias\n\nCormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms. MIT Press.\nKnuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching. Addison-Wesley."
  }
]